{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- LIBRER√çAS ADAPTADAS PARA LSTM Y PERSISTENCIA ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error # üí° NUEVA: MAE\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# --- CONFIGURACI√ìN DE PREDICCI√ìN Y VENTANA LSTM ---\n",
        "TARGET_COLUMN = 'ts'\n",
        "# La predicci√≥n objetivo sigue siendo a H+1\n",
        "PREDICTION_HOUR = 1\n",
        "# La ventana de tiempo/pasos que mirar√° la LSTM para predecir\n",
        "TIME_STEPS = 3\n",
        "# Las caracter√≠sticas que se usar√°n en cada paso de tiempo (timestep) de la LSTM.\n",
        "# üí° NUEVOS FEATURES DE TIEMPO INCLUIDOS: 'hour_sin', 'hour_cos'\n",
        "NEW_FEATURES = ['ts', 'ffInst', 'rrInst', 'radiacionGlobalInst', 'hr', 'p0', 'hour_sin', 'hour_cos']\n",
        "\n",
        "\n",
        "# --- CARGA Y PREPARACI√ìN DEL DATASET √öNICO ---\n",
        "# Se asume la existencia del archivo o se usar√°n los datos de ejemplo (comentado)\n",
        "file_path = 'datos_modificados1.csv'\n",
        "try:\n",
        "    df_raw = pd.read_csv(file_path, sep=',')\n",
        "    print(f\"‚úÖ Archivo '{file_path}' cargado correctamente.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ö†Ô∏è Archivo '{file_path}' no encontrado. Usando datos de ejemplo proporcionados.\")\n",
        "    # Creaci√≥n de DataFrame de ejemplo (a partir de las filas que enviaste)\n",
        "    data = \"\"\"\n",
        "momento,rrInst,hr,p0,qfe1,qff,qnh,radiacionGlobalInst,ts,td,ddInst,ffInst\n",
        "2019-01-01 00:00:00,0.0,28.8,950.4,950.5,1009.1,1011.4,0.0,23.6,4.5,211.3,4.7\n",
        "2019-01-01 00:01:00,0.0,28.8,950.4,950.5,1009.1,1011.4,0.0,23.6,4.5,211.3,4.7\n",
        "2019-01-01 00:02:00,0.0,28.8,950.4,950.5,1009.1,1011.4,0.0,23.6,4.5,211.3,4.7\n",
        "2019-01-01 00:03:00,0.0,28.8,950.4,950.5,1009.1,1011.4,0.0,23.6,4.5,211.3,4.7\n",
        "2019-01-01 00:04:00,0.0,28.8,950.4,950.5,1009.1,1011.4,0.0,23.6,4.5,211.3,4.7\n",
        "2019-01-01 00:05:00,0.0,28.8,950.4,950.5,1009.1,1011.4,0.0,23.6,4.5,211.3,4.7\n",
        "2019-01-01 00:06:00,0.0,28.8,950.4,950.5,1009.1,1011.4,0.0,23.6,4.5,211.3,4.7\n",
        "2019-01-01 00:07:00,0.0,28.8,950.4,950.5,1009.1,1011.4,0.0,23.6,4.5,211.3,4.7\n",
        "2019-01-01 00:08:00,0.0,28.8,950.4,950.5,1009.1,1011.4,0.0,23.6,4.5,211.3,4.7\n",
        "2019-01-01 00:09:00,0.0,28.8,950.4,950.5,1009.1,1011.4,0.0,23.6,4.5,211.3,4.7\n",
        "\"\"\"\n",
        "    from io import StringIO\n",
        "    df_raw = pd.read_csv(StringIO(data), sep=',')\n",
        "    # Para el ejemplo, forzamos un remuestreo m√°s fino para tener m√°s filas\n",
        "    df_raw['momento'] = pd.to_datetime(df_raw['momento'])\n",
        "    df_raw = df_raw.set_index('momento').sort_index()\n",
        "\n",
        "\n",
        "# Convertir la columna de tiempo a formato datetime y establecerla como √≠ndice\n",
        "df_raw['momento'] = pd.to_datetime(df_raw['momento'])\n",
        "df_raw = df_raw.set_index('momento').sort_index()\n",
        "\n",
        "# üí° MEJORA 2: INGENIER√çA DE CARACTER√çSTICAS TEMPORALES (CODIFICACI√ìN SINUSOIDAL)\n",
        "# Esto se hace ANTES del remuestreo para que las horas sean precisas\n",
        "print(\"--- Creando Features C√≠clicos de Tiempo ('hour_sin', 'hour_cos') ---\")\n",
        "timestamp_s = df_raw.index.map(pd.Timestamp.timestamp)\n",
        "hour = df_raw.index.hour # 0 a 23\n",
        "\n",
        "# Codificaci√≥n Sinusoidal para la Hora (Ciclo de 24 horas)\n",
        "day = 24*60*60 # Segundos en un d√≠a\n",
        "df_raw['hour_sin'] = np.sin(2 * np.pi * hour / 24.0)\n",
        "df_raw['hour_cos'] = np.cos(2 * np.pi * hour / 24.0)\n",
        "\n",
        "# Filtramos solo las columnas que vamos a usar\n",
        "# (Se incluyen los nuevos features de tiempo)\n",
        "df_raw = df_raw[[f for f in NEW_FEATURES if f in df_raw.columns]]\n",
        "df_raw = df_raw.dropna() # Se eliminan NaNs antes del remuestreo\n",
        "\n",
        "\n",
        "# üí° MEJORA 1: REMUESTREO (RESAMPLING) A FRECUENCIA HORARIA ('H')\n",
        "print(\"--- Remuestreando los datos a frecuencia Horaria (H) ---\")\n",
        "\n",
        "# Definir reglas de agregaci√≥n (media para la mayor√≠a, suma para lluvia, sin cambiar sin/cos)\n",
        "resample_rules = {col: 'mean' for col in df_raw.columns}\n",
        "if 'rrInst' in resample_rules:\n",
        "    resample_rules['rrInst'] = 'sum'\n",
        "if 'hour_sin' in resample_rules:\n",
        "    resample_rules['hour_sin'] = 'mean'\n",
        "if 'hour_cos' in resample_rules:\n",
        "    resample_rules['hour_cos'] = 'mean'\n",
        "\n",
        "\n",
        "df = df_raw.resample('H').agg(resample_rules)\n",
        "df = df.dropna()\n",
        "print(f\"Filas despu√©s de Remuestreo y NaN: {len(df)}\")\n",
        "\n",
        "\n",
        "# --- ESCALADO DE DATOS (CRUCIAL PARA REDES NEURONALES) ---\n",
        "print(\"--- Escalando los datos (MinMaxScaler) ---\")\n",
        "# Escalar todas las caracter√≠sticas usadas\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(df.values)\n",
        "df_scaled = pd.DataFrame(scaled_data, columns=df.columns, index=df.index)\n",
        "\n",
        "# Crear un escalador solo para la columna 'ts' (target) para el desescalado final.\n",
        "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "target_scaler.fit(df[[TARGET_COLUMN]])\n",
        "\n",
        "\n",
        "# --- FUNCI√ìN PARA CREAR SECUENCIAS LSTM (Transformaci√≥n 3D) ---\n",
        "def create_sequences(data, target, time_steps, target_hour):\n",
        "    \"\"\"\n",
        "    Crea las secuencias de entrada (X, 3D) y salida (y, 1D) para la LSTM.\n",
        "    X: [muestras, pasos_de_tiempo, caracter√≠sticas]\n",
        "    y: [muestras] (el valor de 'target' en el momento T + target_hour)\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - time_steps - target_hour + 1):\n",
        "        # Secuencia X: Mirar atr√°s 'time_steps' pasos. Incluye TODAS las caracter√≠sticas.\n",
        "        seq_x = data.iloc[i:(i + time_steps)].values\n",
        "        X.append(seq_x)\n",
        "\n",
        "        # Objetivo y: El valor de 'target' en el momento futuro (T + target_hour)\n",
        "        seq_y = target.iloc[i + time_steps + target_hour - 1]\n",
        "        y.append(seq_y)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# 1. Definir las caracter√≠sticas de entrada para la secuencia LSTM\n",
        "X_features = df_scaled[NEW_FEATURES]\n",
        "y_target = df_scaled[TARGET_COLUMN]\n",
        "\n",
        "# 2. Transformar los datos de series de tiempo a la estructura 3D requerida por LSTM\n",
        "X, y = create_sequences(X_features, y_target, TIME_STEPS, PREDICTION_HOUR)\n",
        "\n",
        "# --- DIVISI√ìN DE DATOS EN ENTRENAMIENTO Y PRUEBA (Temporal) ---\n",
        "train_size = int(len(X) * 0.8)\n",
        "\n",
        "X_train = X[:train_size]\n",
        "X_test = X[train_size:]\n",
        "y_train = y[:train_size]\n",
        "y_test = y[train_size:]\n",
        "\n",
        "# 3. Imprimir las dimensiones 3D de la entrada y 1D de la salida\n",
        "print(f\"\\n--- Dimensiones de los Datos para LSTM ---\")\n",
        "print(f\"N√∫mero de caracter√≠sticas de entrada: **{X_train.shape[2]}**\")\n",
        "print(f\"Caracter√≠sticas utilizadas: {X_features.columns.tolist()}\")\n",
        "print(f\"X_train shape: {X_train.shape} (muestras, pasos_tiempo, caracter√≠sticas)\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## --- MODELO LSTM Y ENTRENAMIENTO ---\n",
        "print(\"\\n--- Construyendo y Entrenando el modelo LSTM ---\")\n",
        "\n",
        "# Definici√≥n del modelo\n",
        "model = Sequential([\n",
        "    # Capa LSTM: 50 unidades, input_shape: (pasos_tiempo, caracter√≠sticas)\n",
        "    LSTM(50, activation='relu', input_shape=(TIME_STEPS, X_train.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    # Capa de salida: 1 neurona (para regresi√≥n)\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compilaci√≥n del modelo\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Callbacks: Detener el entrenamiento si la p√©rdida no mejora despu√©s de 5 √©pocas\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Entrenamiento\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## --- EVALUACI√ìN Y DESESCALADO DEL MODELO ---\n",
        "# 1. Predicci√≥n\n",
        "y_pred_scaled = model.predict(X_test)\n",
        "\n",
        "# 2. Desescalar las predicciones y los valores reales\n",
        "y_test_unscaled = target_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "y_pred_unscaled = target_scaler.inverse_transform(y_pred_scaled)\n",
        "\n",
        "# 3. C√°lculo de m√©tricas\n",
        "mse = mean_squared_error(y_test_unscaled, y_pred_unscaled)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test_unscaled, y_pred_unscaled)\n",
        "mae = mean_absolute_error(y_test_unscaled, y_pred_unscaled) # üí° NUEVA: MAE\n",
        "\n",
        "print(f\"\\n--- Resultados de la Predicci√≥n (H+{PREDICTION_HOUR}) con LSTM (con Features de Tiempo) ---\")\n",
        "print(f\"Error Absoluto Medio (MAE): {mae:.2f} ¬∞C\") # üí° NUEVA\n",
        "print(f\"Error Cuadr√°tico Medio (MSE): {mse:.2f}\")\n",
        "print(f\"Ra√≠z del Error Cuadr√°tico Medio (RMSE): {rmse:.2f} ¬∞C\")\n",
        "print(f\"Coeficiente de Determinaci√≥n (R¬≤): {r2:.4f}\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## --- PERSISTENCIA DEL MODELO (GUARDAR) ---\n",
        "model_filename = f'lstm_model_H{PREDICTION_HOUR}_hourly_timefeatures.h5'\n",
        "scaler_filename = f'target_scaler_H{PREDICTION_HOUR}_hourly_timefeatures.joblib'\n",
        "model.save(model_filename)\n",
        "joblib.dump(target_scaler, scaler_filename)\n",
        "\n",
        "print(f\"\\n‚úÖ Modelo guardado como: {model_filename}\")\n",
        "print(f\"‚úÖ Escalador guardado como: {scaler_filename}\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## --- GR√ÅFICOS ---\n",
        "# 1. Gr√°fico de Predicci√≥n vs. Real\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Determinar el √≠ndice de tiempo correcto para los datos de prueba\n",
        "# Ajustar para la longitud de X_test (que es len(y_test)) y el desfase de PREDICTION_HOUR\n",
        "original_test_index = df.index[len(df) - len(y_test) - PREDICTION_HOUR:]\n",
        "test_index = original_test_index[PREDICTION_HOUR:] # El √≠ndice real de la predicci√≥n H+1\n",
        "\n",
        "plt.plot(test_index, y_test_unscaled, label='Temperatura Real (ts)', alpha=0.7, color='blue')\n",
        "plt.plot(test_index, y_pred_unscaled, label=f'Predicci√≥n LSTM (H+{PREDICTION_HOUR})', alpha=0.7, color='red')\n",
        "plt.title(f'Predicci√≥n de Temperatura (ts) a H+{PREDICTION_HOUR} con LSTM (Con Features de Tiempo)')\n",
        "plt.xlabel('momento')\n",
        "plt.ylabel('Temperatura (¬∞C)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Gr√°fico de P√©rdida (Loss) durante el entrenamiento\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='P√©rdida de Entrenamiento (Loss)', color='orange')\n",
        "plt.plot(history.history['val_loss'], label='P√©rdida de Validaci√≥n (Val_Loss)', color='purple')\n",
        "plt.title('Curva de P√©rdida del Modelo LSTM')\n",
        "plt.xlabel('√âpoca')\n",
        "plt.ylabel('P√©rdida (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 3. Gr√°fico de Error Absoluto Medio (MAE) üí° NUEVO\n",
        "# NOTA: Como la p√©rdida (loss) es MSE, no podemos graficar directamente la curva de MAE del entrenamiento,\n",
        "# pero graficaremos la distribuci√≥n de los errores absolutos en la prueba.\n",
        "plt.figure(figsize=(10, 6))\n",
        "absolute_errors = np.abs(y_test_unscaled - y_pred_unscaled)\n",
        "plt.hist(absolute_errors, bins=50, color='darkgreen', alpha=0.7, edgecolor='black')\n",
        "plt.axvline(mae, color='red', linestyle='dashed', linewidth=2, label=f'MAE = {mae:.2f} ¬∞C')\n",
        "plt.title('Distribuci√≥n de Errores Absolutos en el Conjunto de Prueba')\n",
        "plt.xlabel('Error Absoluto ( |Real - Predicci√≥n| ) (¬∞C)')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.legend()\n",
        "plt.grid(True, axis='y', alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xgo4v7fvjN9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LIBRER√çAS ADAPTADAS PARA RANDOM FOREST Y PERSISTENCIA ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# --- CONFIGURACI√ìN DEL MODELO ---\n",
        "prediction_hour = 1 # Predicci√≥n a 1 hora (H+1)\n",
        "\n",
        "# MODO EXPERIMENTAL: Desactivar lags para probar modelo sin persistencia\n",
        "USE_LAGS = True  # ‚Üê Cambiar a True para activar lags\n",
        "\n",
        "# Caracter√≠sticas que queremos \"desfasar\" (Lags)\n",
        "lag_features = ['ts', 'hr', 'p0']\n",
        "lag_steps = [1, 2, 3] # Desfases de 1, 2 y 3 horas\n",
        "\n",
        "# --- CARGA Y PREPARACI√ìN DEL DATASET √öNICO ---\n",
        "\n",
        "file_path = \"datos_modificados1.csv\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    raise FileNotFoundError(f\"No se encuentra: {file_path}\")\n",
        "\n",
        "df_raw = pd.read_csv(\"datos_modificados1.csv\")\n",
        "print(df_raw.shape)\n",
        "print(df_raw.columns)\n",
        "\n",
        "print(\"VERIFICACI√ìN DEL DATASET\")\n",
        "\n",
        "# 1. Mostrar primeras filas\n",
        "print(\"Primeras 5 filas del dataset:\")\n",
        "print(df_raw.head(), \"\\n\")\n",
        "\n",
        "# 2. Tipos de datos\n",
        "print(\"Tipos de datos:\")\n",
        "print(df_raw.dtypes, \"\\n\")\n",
        "\n",
        "# 3. Conteo de nulos por columna\n",
        "print(\"Valores nulos por columna:\")\n",
        "print(df_raw.isnull().sum(), \"\\n\")\n",
        "\n",
        "# 4. Filas completamente duplicadas\n",
        "print(f\"Filas duplicadas: {df_raw.duplicated().sum()}\\n\")\n",
        "\n",
        "# 5. Rango y ejemplo de la columna 'momento'\n",
        "if 'momento' in df_raw.columns:\n",
        "    print(\"Columna 'momento' detectada:\")\n",
        "    print(\" - Primer valor:\", df_raw['momento'].iloc[0])\n",
        "    print(\" - √öltimo valor:\", df_raw['momento'].iloc[-1])\n",
        "else:\n",
        "    print(\"ERROR: No existe la columna 'momento' en el dataset\\n\")\n",
        "\n",
        "# 6. Revisar si columnas clave existen\n",
        "columnas_esperadas = ['momento', 'ts', 'hr', 'radiacionGlobalInst', 'ffInst', 'rrInst', 'p0']\n",
        "print(\"Verificando columnas necesarias...\")\n",
        "for col in columnas_esperadas:\n",
        "    if col not in df_raw.columns:\n",
        "        print(f\"Falta columna requerida: {col}\")\n",
        "    else:\n",
        "        print(f\"{col} OK\")\n",
        "print(\"\")\n",
        "\n",
        "# 7. Estad√≠sticas b√°sicas\n",
        "print(\"Estad√≠sticas num√©ricas:\")\n",
        "print(df_raw.describe(), \"\\n\")\n",
        "\n",
        "# 8. Valores fuera de rango sospechosos\n",
        "if 'ts' in df_raw.columns:\n",
        "    ts_min, ts_max = df_raw['ts'].min(), df_raw['ts'].max()\n",
        "    print(f\"Temperatura: min={ts_min}¬∞C | max={ts_max}¬∞C\")\n",
        "    if ts_min < -20 or ts_max > 50:\n",
        "        print(\"Valores at√≠picos detectados en temperatura\\n\")\n",
        "\n",
        "if 'hr' in df_raw.columns:\n",
        "    hr_min, hr_max = df_raw['hr'].min(), df_raw['hr'].max()\n",
        "    print(f\"Humedad relativa: min={hr_min}% | max={hr_max}%\")\n",
        "    if hr_min < 0 or hr_max > 100:\n",
        "        print(\"Humedad fuera de rango (0-100%)\\n\")\n",
        "\n",
        "print(\"\\n======================\")\n",
        "print(\"FIN DE VERIFICACI√ìN DEL DATASET\")\n",
        "print(\"======================\\n\")\n",
        "\n",
        "# Convertir la columna de tiempo a formato datetime y establecerla como √≠ndice\n",
        "df_raw['momento'] = pd.to_datetime(df_raw['momento'])\n",
        "df_raw = df_raw.set_index('momento').sort_index()\n",
        "df_raw = df_raw.dropna()\n",
        "\n",
        "print(\"--- Remuestreando los datos a frecuencia Horaria (h) ---\")\n",
        "\n",
        "# Definir c√≥mo se agregan las columnas en el remuestreo horario\n",
        "resample_rules = {\n",
        "    'ts': 'mean', # Temperatura (Media horaria)\n",
        "    'hr': 'mean', # Humedad (Media horaria)\n",
        "    'radiacionGlobalInst': 'mean', # Radiaci√≥n (Media horaria)\n",
        "    'ffInst': 'mean', # Velocidad del Viento (Media horaria)\n",
        "    'rrInst': 'sum', # Lluvia (Suma horaria)\n",
        "    'p0': 'mean'\n",
        "}\n",
        "\n",
        "# Aplicar remuestreo (corregido 'h' min√∫scula)\n",
        "df = df_raw.resample('h').agg(resample_rules)\n",
        "\n",
        "# --- INGENIER√çA DE CARACTER√çSTICAS TEMPORALES (USANDO 'momento') ---\n",
        "print(\"--- Extrayendo caracter√≠sticas temporales (Hora y D√≠a del A√±o) ---\")\n",
        "df['hour'] = df.index.hour\n",
        "df['dayofyear'] = df.index.dayofyear\n",
        "\n",
        "# --- DEFINICI√ìN DE CARACTER√çSTICAS (X) Y OBJETIVO (Y) ---\n",
        "target = 'ts'\n",
        "\n",
        "# Caracter√≠sticas iniciales\n",
        "features = ['hr', 'p0', 'hour', 'dayofyear']\n",
        "\n",
        "# Crear las caracter√≠sticas desfasadas (lags) - SOLO SI EST√Å ACTIVADO\n",
        "if USE_LAGS:\n",
        "    print(\"Modo: CON Lags (ts_lag_1h, ts_lag_2h, ts_lag_3h...)\")\n",
        "    for feature in lag_features:\n",
        "        for h in lag_steps:\n",
        "            new_col_name = f'{feature}lag{h}h'\n",
        "            df[new_col_name] = df[feature].shift(h)\n",
        "            features.append(new_col_name)\n",
        "else:\n",
        "    print(\"üî∏ Modo: SIN Lags (solo variables f√≠sicas y temporales)\")\n",
        "\n",
        "# Crear la columna objetivo desfasada\n",
        "df[f'ts_future_H{prediction_hour}'] = df[target].shift(-prediction_hour)\n",
        "\n",
        "# Filtrar el DataFrame final y eliminar filas nulas\n",
        "df_model = df[features + [f'ts_future_H{prediction_hour}']].dropna()\n",
        "\n",
        "X = df_model[features]\n",
        "y = df_model[f'ts_future_H{prediction_hour}']\n",
        "\n",
        "print(f\"\\n--- Caracter√≠sticas (X) a usar en el modelo ---\")\n",
        "print(X.columns.tolist())\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# --- DIVISI√ìN DE DATOS: 60% TRAIN, 20% VALIDATION, 20% TEST ---\n",
        "print(\"\\n--- Divisi√≥n de Datos: 60% Train | 20% Validation | 20% Test ---\")\n",
        "\n",
        "total_size = len(df_model)\n",
        "train_size = int(total_size * 0.6)\n",
        "val_size = int(total_size * 0.2)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "# Divisi√≥n temporal (respetando el orden cronol√≥gico)\n",
        "X_train = X[:train_size]\n",
        "X_val = X[train_size:train_size + val_size]\n",
        "X_test = X[train_size + val_size:]\n",
        "\n",
        "y_train = y[:train_size]\n",
        "y_val = y[train_size:train_size + val_size]\n",
        "y_test = y[train_size + val_size:]\n",
        "\n",
        "print(f\"Train:      {len(X_train):6d} filas ({len(X_train)/total_size*100:.1f}%)\")\n",
        "print(f\"Validation: {len(X_val):6d} filas ({len(X_val)/total_size*100:.1f}%)\")\n",
        "print(f\"Test:       {len(X_test):6d} filas ({len(X_test)/total_size*100:.1f}%)\")\n",
        "print(f\"Total:      {total_size:6d} filas\")\n",
        "\n",
        "# --- MODELO RANDOM FOREST REGRESSOR ---\n",
        "print(\"\\n--- Entrenando Random Forest Regressor ---\")\n",
        "print(\"Hiperpar√°metros: {'max_depth': 20, 'min_samples_leaf': 5, 'n_estimators': 100}\")\n",
        "\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=20,\n",
        "    min_samples_leaf=5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# --- FUNCI√ìN AUXILIAR PARA CALCULAR M√âTRICAS ---\n",
        "def calcular_metricas(y_real, y_pred, set_name):\n",
        "    \"\"\"Calcula y muestra MAE, MSE y R¬≤ para un conjunto de datos\"\"\"\n",
        "    mae = mean_absolute_error(y_real, y_pred)\n",
        "    mse = mean_squared_error(y_real, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_real, y_pred)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"M√âTRICAS - {set_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"MAE  (Mean Absolute Error):       {mae:.4f} ¬∞C\")\n",
        "    print(f\"MSE  (Mean Squared Error):        {mse:.4f} ¬∞C¬≤\")\n",
        "    print(f\"RMSE (Root Mean Squared Error):   {rmse:.4f} ¬∞C\")\n",
        "    print(f\"R¬≤   (Coeficiente Determinaci√≥n): {r2:.4f}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    return {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2}\n",
        "\n",
        "# --- EVALUACI√ìN EN CONJUNTO DE ENTRENAMIENTO ---\n",
        "y_train_pred = rf_model.predict(X_train)\n",
        "metricas_train = calcular_metricas(y_train, y_train_pred, \"TRAIN SET\")\n",
        "\n",
        "# --- EVALUACI√ìN EN CONJUNTO DE VALIDACI√ìN ---\n",
        "y_val_pred = rf_model.predict(X_val)\n",
        "metricas_val = calcular_metricas(y_val, y_val_pred, \"VALIDATION SET\")\n",
        "\n",
        "# --- EVALUACI√ìN EN CONJUNTO DE PRUEBA ---\n",
        "y_test_pred = rf_model.predict(X_test)\n",
        "metricas_test = calcular_metricas(y_test, y_test_pred, \"TEST SET\")\n",
        "\n",
        "# --- DETECCI√ìN DE OVERFITTING ---\n",
        "print(\"\\n--- üîç An√°lisis de Overfitting ---\")\n",
        "diff_train_val_r2 = metricas_train['R2'] - metricas_val['R2']\n",
        "diff_train_val_mae = metricas_val['MAE'] - metricas_train['MAE']\n",
        "\n",
        "print(f\"Diferencia R¬≤ (Train - Val):  {diff_train_val_r2:.4f}\")\n",
        "print(f\"Diferencia MAE (Val - Train): {diff_train_val_mae:.4f} ¬∞C\")\n",
        "\n",
        "if diff_train_val_r2 > 0.05:\n",
        "    print(\"ADVERTENCIA: Posible overfitting detectado (R¬≤ train >> R¬≤ val)\")\n",
        "elif diff_train_val_mae > 0.2:\n",
        "    print(\"ADVERTENCIA: Posible overfitting detectado (MAE val >> MAE train)\")\n",
        "else:\n",
        "    print(\"No se detecta overfitting significativo\")\n",
        "\n",
        "# --- PERSISTENCIA DEL MODELO ---\n",
        "model_filename = f'random_forest_H{prediction_hour}_hourly_full_features.joblib'\n",
        "joblib.dump(rf_model, model_filename)\n",
        "joblib.dump(X.columns.tolist(), f'features_H{prediction_hour}_hourly_full_features.joblib')\n",
        "\n",
        "print(f\"\\nModelo guardado como: {model_filename}\")\n",
        "print(f\"Lista de features guardada como: features_H{prediction_hour}_hourly_full_features.joblib\")\n",
        "\n",
        "# --- IMPORTANCIA DE LAS CARACTER√çSTICAS ---\n",
        "print(\"\\n--- Importancia de las Caracter√≠sticas (Feature Importance) ---\")\n",
        "feature_importances = pd.Series(rf_model.feature_importances_, index=features)\n",
        "feature_importances_sorted = feature_importances.sort_values(ascending=False)\n",
        "print(feature_importances_sorted)\n",
        "\n",
        "# --- VALIDACI√ìN CRUZADA TEMPORAL (SOLO EN TRAIN) ---\n",
        "print(\"\\n--- Validaci√≥n Cruzada Temporal (Time Series CV sobre Train Set) ---\")\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "cv_mae_scores = []\n",
        "cv_mse_scores = []\n",
        "cv_r2_scores = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\n",
        "    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "    rf_temp = RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=20,\n",
        "        min_samples_leaf=5,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    rf_temp.fit(X_fold_train, y_fold_train)\n",
        "\n",
        "    y_fold_pred = rf_temp.predict(X_fold_val)\n",
        "    fold_mae = mean_absolute_error(y_fold_val, y_fold_pred)\n",
        "    fold_mse = mean_squared_error(y_fold_val, y_fold_pred)\n",
        "    fold_rmse = np.sqrt(fold_mse)\n",
        "    fold_r2 = r2_score(y_fold_val, y_fold_pred)\n",
        "\n",
        "    cv_mae_scores.append(fold_mae)\n",
        "    cv_mse_scores.append(fold_mse)\n",
        "    cv_r2_scores.append(fold_r2)\n",
        "\n",
        "    print(f\"Fold {fold}: MAE = {fold_mae:.4f}¬∞C | MSE = {fold_mse:.4f} | R¬≤ = {fold_r2:.4f}\")\n",
        "\n",
        "print(f\"\\nPromedio CV: MAE = {np.mean(cv_mae_scores):.4f}¬∞C ¬± {np.std(cv_mae_scores):.4f}\")\n",
        "print(f\"Promedio CV: MSE = {np.mean(cv_mse_scores):.4f} ¬± {np.std(cv_mse_scores):.4f}\")\n",
        "print(f\"Promedio CV: R¬≤  = {np.mean(cv_r2_scores):.4f} ¬± {np.std(cv_r2_scores):.4f}\")\n",
        "\n",
        "# --- VALIDACI√ìN POR ESTACI√ìN DEL A√ëO (EN TEST SET) ---\n",
        "print(\"\\n--- Validaci√≥n por Estaciones del A√±o (Test Set) ---\")\n",
        "\n",
        "def get_season(month):\n",
        "    if month in [12, 1, 2]:\n",
        "        return 'Invierno'\n",
        "    elif month in [3, 4, 5]:\n",
        "        return 'Primavera'\n",
        "    elif month in [6, 7, 8]:\n",
        "        return 'Verano'\n",
        "    else:\n",
        "        return 'Oto√±o'\n",
        "\n",
        "y_test_with_season = pd.DataFrame({\n",
        "    'real': y_test.values,\n",
        "    'pred': y_test_pred,\n",
        "    'season': [get_season(idx.month) for idx in y_test.index]\n",
        "})\n",
        "\n",
        "for season in ['Invierno', 'Primavera', 'Verano', 'Oto√±o']:\n",
        "    mask = y_test_with_season['season'] == season\n",
        "    if mask.sum() > 0:\n",
        "        season_mae = mean_absolute_error(\n",
        "            y_test_with_season.loc[mask, 'real'],\n",
        "            y_test_with_season.loc[mask, 'pred']\n",
        "        )\n",
        "        season_mse = mean_squared_error(\n",
        "            y_test_with_season.loc[mask, 'real'],\n",
        "            y_test_with_season.loc[mask, 'pred']\n",
        "        )\n",
        "        season_r2 = r2_score(\n",
        "            y_test_with_season.loc[mask, 'real'],\n",
        "            y_test_with_season.loc[mask, 'pred']\n",
        "        )\n",
        "        print(f\"{season:12s}: MAE = {season_mae:.4f}¬∞C | MSE = {season_mse:.4f} | R¬≤ = {season_r2:.4f} ({mask.sum()} muestras)\")\n",
        "\n",
        "# --- DETECCI√ìN DE VALORES EXTREMOS ---\n",
        "print(\"\\n--- Detecci√≥n de Predicciones en Condiciones Extremas (Test Set) ---\")\n",
        "\n",
        "temp_p10 = y_train.quantile(0.10)\n",
        "temp_p90 = y_train.quantile(0.90)\n",
        "\n",
        "extreme_cold_mask = y_test < temp_p10\n",
        "extreme_hot_mask = y_test > temp_p90\n",
        "\n",
        "if extreme_cold_mask.sum() > 0:\n",
        "    cold_mae = mean_absolute_error(y_test[extreme_cold_mask], y_test_pred[extreme_cold_mask])\n",
        "    cold_mse = mean_squared_error(y_test[extreme_cold_mask], y_test_pred[extreme_cold_mask])\n",
        "    cold_r2 = r2_score(y_test[extreme_cold_mask], y_test_pred[extreme_cold_mask])\n",
        "    print(f\"‚ùÑ  Fr√≠o extremo (<{temp_p10:.1f}¬∞C): MAE = {cold_mae:.4f}¬∞C | MSE = {cold_mse:.4f} | R¬≤ = {cold_r2:.4f} ({extreme_cold_mask.sum()} casos)\")\n",
        "\n",
        "if extreme_hot_mask.sum() > 0:\n",
        "    hot_mae = mean_absolute_error(y_test[extreme_hot_mask], y_test_pred[extreme_hot_mask])\n",
        "    hot_mse = mean_squared_error(y_test[extreme_hot_mask], y_test_pred[extreme_hot_mask])\n",
        "    hot_r2 = r2_score(y_test[extreme_hot_mask], y_test_pred[extreme_hot_mask])\n",
        "    print(f\"Calor extremo (>{temp_p90:.1f}¬∞C): MAE = {hot_mae:.4f}¬∞C | MSE = {hot_mse:.4f} | R¬≤ = {hot_r2:.4f} ({extreme_hot_mask.sum()} casos)\")\n",
        "\n",
        "# --- AN√ÅLISIS DE ERRORES POR HORA ---\n",
        "print(\"\\n--- An√°lisis de Sesgo Temporal en Errores (Test Set) ---\")\n",
        "\n",
        "errors = y_test - y_test_pred\n",
        "errors_by_hour = pd.DataFrame({\n",
        "    'error': errors.values,\n",
        "    'hour': [idx.hour for idx in y_test.index]\n",
        "}).groupby('hour')['error'].agg(['mean', 'std'])\n",
        "\n",
        "print(\"\\nError promedio por hora del d√≠a:\")\n",
        "print(errors_by_hour.round(4))\n",
        "\n",
        "problematic_hours = errors_by_hour[abs(errors_by_hour['mean']) > 0.5]\n",
        "if not problematic_hours.empty:\n",
        "    print(f\"\\nHoras con sesgo >0.5¬∞C: {problematic_hours.index.tolist()}\")\n",
        "else:\n",
        "    print(\"\\nNo se detectaron horas con sesgo significativo\")\n",
        "\n",
        "# --- GR√ÅFICOS ---\n",
        "print(\"\\n--- Generando Gr√°ficos ---\")\n",
        "\n",
        "# Gr√°fico 1: Predicci√≥n vs Real (Test Set)\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(y_test.index, y_test.values, label='Temperatura Real', alpha=0.7, linewidth=1.5)\n",
        "plt.plot(y_test.index, y_test_pred, label=f'Predicci√≥n RF (H+{prediction_hour})', alpha=0.7, linewidth=1.5)\n",
        "plt.title(f'Predicci√≥n vs Real - Test Set (MAE: {metricas_test[\"MAE\"]:.4f}¬∞C, R¬≤: {metricas_test[\"R2\"]:.4f})')\n",
        "plt.xlabel('momento')\n",
        "plt.ylabel('Temperatura (¬∞C)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'prediccion_test_H{prediction_hour}.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# Gr√°fico 2: Importancia de Caracter√≠sticas\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(feature_importances_sorted.index, feature_importances_sorted.values, color='teal')\n",
        "plt.title('Importancia de las Caracter√≠sticas en Random Forest')\n",
        "plt.xlabel('Importancia')\n",
        "plt.ylabel('Caracter√≠stica')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'feature_importance_H{prediction_hour}.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# Gr√°fico 3: Scatter Plot (Test Set)\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(y_test, y_test_pred, alpha=0.5, color='darkred', s=10)\n",
        "max_val = max(y_test.max(), y_test_pred.max())\n",
        "min_val = min(y_test.min(), y_test_pred.min())\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Predicci√≥n Perfecta')\n",
        "plt.title(f'Scatter Plot: Real vs Predicci√≥n (Test Set)\\nMAE: {metricas_test[\"MAE\"]:.4f}¬∞C | R¬≤: {metricas_test[\"R2\"]:.4f}')\n",
        "plt.xlabel('Temperatura Real (¬∞C)')\n",
        "plt.ylabel('Temperatura Predicha (¬∞C)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'scatter_test_H{prediction_hour}.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# Gr√°fico 4: Comparaci√≥n de M√©tricas (Train/Val/Test)\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "sets = ['Train', 'Validation', 'Test']\n",
        "mae_values = [metricas_train['MAE'], metricas_val['MAE'], metricas_test['MAE']]\n",
        "mse_values = [metricas_train['MSE'], metricas_val['MSE'], metricas_test['MSE']]\n",
        "r2_values = [metricas_train['R2'], metricas_val['R2'], metricas_test['R2']]\n",
        "\n",
        "axes[0].bar(sets, mae_values, color=['blue', 'orange', 'green'])\n",
        "axes[0].set_title('MAE por Conjunto')\n",
        "axes[0].set_ylabel('MAE (¬∞C)')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].bar(sets, mse_values, color=['blue', 'orange', 'green'])\n",
        "axes[1].set_title('MSE por Conjunto')\n",
        "axes[1].set_ylabel('MSE (¬∞C¬≤)')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].bar(sets, r2_values, color=['blue', 'orange', 'green'])\n",
        "axes[2].set_title('R¬≤ por Conjunto')\n",
        "axes[2].set_ylabel('R¬≤')\n",
        "axes[2].set_ylim([0.9, 1.0])\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'metricas_comparacion_H{prediction_hour}.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"Gr√°ficos guardados exitosamente\")\n",
        "print(\"\\nArchivos generados:\")\n",
        "print(f\"  - {model_filename}\")\n",
        "print(f\"  - features_H{prediction_hour}_hourly_full_features.joblib\")\n",
        "print(f\"  - prediccion_test_H{prediction_hour}.png\")\n",
        "print(f\"  - feature_importance_H{prediction_hour}.png\")\n",
        "print(f\"  - scatter_test_H{prediction_hour}.png\")\n",
        "print(f\"  - metricas_comparacion_H{prediction_hour}.png\")\n",
        "\n",
        "# =======================================================\n",
        "# 4. Generaci√≥n de Artefactos (Para CML)\n",
        "# =======================================================\n",
        "\n",
        "# 4.1. Guardar el Modelo Entrenado (rf_model.pkl)\n",
        "model_filename = 'rf_model.pkl'\n",
        "joblib.dump(rf_model, model_filename)\n",
        "print(f\"Modelo guardado como {model_filename}.\")\n",
        "\n",
        "# 4.2. Generar el Gr√°fico de Predicci√≥n (prediction_plot.png)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.6, color='darkblue')\n",
        "max_val = max(y_test.max(), y_pred.max())\n",
        "min_val = min(y_test.min(), y_pred.min())\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
        "plt.title('Random Forest: Predicci√≥n vs. Temperatura Real')\n",
        "plt.xlabel('Temperatura M√°xima Real')\n",
        "plt.ylabel('Temperatura M√°xima Predicha')\n",
        "plt.grid(True)\n",
        "plot_filename = 'prediction_plot.png'\n",
        "plt.savefig(plot_filename)\n",
        "print(f\"Gr√°fico de predicci√≥n guardado como {plot_filename}.\")\n",
        "\n",
        "# 4.3. Guardar las M√©tricas (metrics.txt)\n",
        "metrics_filename = 'metrics.txt'\n",
        "with open(metrics_filename, 'w') as f:\n",
        "    f.write(\"Random Forest Regressor - Predicci√≥n de Temperatura M√°xima\\n\")\n",
        "    f.write(\"-\" * 50 + \"\\n\")\n",
        "    f.write(f\"Caracter√≠sticas utilizadas: {features}\\n\")\n",
        "    f.write(f\"MSE (Error Cuadr√°tico Medio): {mse:.2f}\\n\")\n",
        "    f.write(f\"R2 Score (Coeficiente de Determinaci√≥n): {r2:.4f}\\n\")\n",
        "print(f\"M√©tricas guardadas en {metrics_filename}.\")\n",
        "\n",
        "# =======================================================\n",
        "# Fin del Script para CML\n",
        "# =======================================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKWkERgHhd1G",
        "outputId": "e747c95f-f7ea-4099-e228-d029a6a2e5f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "(16774505, 12)\n",
            "Index(['momento', 'rrInst', 'hr', 'p0', 'qfe1', 'qff', 'qnh',\n",
            "       'radiacionGlobalInst', 'ts', 'td', 'ddInst', 'ffInst'],\n",
            "      dtype='object')\n",
            "VERIFICACI√ìN DEL DATASET\n",
            "Primeras 5 filas del dataset:\n",
            "               momento  rrInst    hr     p0   qfe1     qff     qnh  \\\n",
            "0  2019-01-01 00:00:00     0.0  28.8  950.4  950.5  1009.1  1011.4   \n",
            "1  2019-01-01 00:01:00     0.0  28.8  950.4  950.5  1009.1  1011.4   \n",
            "2  2019-01-01 00:02:00     0.0  28.8  950.4  950.5  1009.1  1011.4   \n",
            "3  2019-01-01 00:03:00     0.0  28.8  950.4  950.5  1009.1  1011.4   \n",
            "4  2019-01-01 00:04:00     0.0  28.8  950.4  950.5  1009.1  1011.4   \n",
            "\n",
            "   radiacionGlobalInst    ts   td  ddInst  ffInst  \n",
            "0                  0.0  23.6  4.5   211.3     4.7  \n",
            "1                  0.0  23.6  4.5   211.3     4.7  \n",
            "2                  0.0  23.6  4.5   211.3     4.7  \n",
            "3                  0.0  23.6  4.5   211.3     4.7  \n",
            "4                  0.0  23.6  4.5   211.3     4.7   \n",
            "\n",
            "Tipos de datos:\n",
            "momento                 object\n",
            "rrInst                 float64\n",
            "hr                     float64\n",
            "p0                     float64\n",
            "qfe1                   float64\n",
            "qff                    float64\n",
            "qnh                    float64\n",
            "radiacionGlobalInst    float64\n",
            "ts                     float64\n",
            "td                     float64\n",
            "ddInst                 float64\n",
            "ffInst                 float64\n",
            "dtype: object \n",
            "\n",
            "Valores nulos por columna:\n",
            "momento                0\n",
            "rrInst                 0\n",
            "hr                     0\n",
            "p0                     0\n",
            "qfe1                   0\n",
            "qff                    0\n",
            "qnh                    0\n",
            "radiacionGlobalInst    0\n",
            "ts                     0\n",
            "td                     0\n",
            "ddInst                 0\n",
            "ffInst                 0\n",
            "dtype: int64 \n",
            "\n",
            "Filas duplicadas: 387\n",
            "\n",
            "Columna 'momento' detectada:\n",
            " - Primer valor: 2019-01-01 00:00:00\n",
            " - √öltimo valor: 2025-11-01 00:00:00\n",
            "Verificando columnas necesarias...\n",
            "momento OK\n",
            "ts OK\n",
            "hr OK\n",
            "radiacionGlobalInst OK\n",
            "ffInst OK\n",
            "rrInst OK\n",
            "p0 OK\n",
            "\n",
            "Estad√≠sticas num√©ricas:\n",
            "             rrInst            hr            p0          qfe1           qff  \\\n",
            "count  1.677450e+07  1.677450e+07  1.677450e+07  1.677450e+07  1.677450e+07   \n",
            "mean   4.145577e-05  6.173711e+01  9.555893e+02  9.557502e+02  1.017092e+03   \n",
            "std    2.235783e-03  1.754522e+01  2.590081e+00  2.602601e+00  3.893734e+00   \n",
            "min    0.000000e+00  6.300000e+00  5.059970e+02  5.060900e+02  5.383000e+02   \n",
            "25%    0.000000e+00  6.400000e+01  9.557440e+02  9.559000e+02  1.017300e+03   \n",
            "50%    0.000000e+00  6.870000e+01  9.565910e+02  9.567620e+02  1.018720e+03   \n",
            "75%    0.000000e+00  6.870000e+01  9.565910e+02  9.567620e+02  1.018720e+03   \n",
            "max    9.000000e-01  1.000000e+02  9.707700e+02  9.710440e+02  1.036770e+03   \n",
            "\n",
            "                qnh  radiacionGlobalInst            ts            td  \\\n",
            "count  1.677450e+07         1.677450e+07  1.677450e+07  1.677450e+07   \n",
            "mean   1.017527e+03         4.548016e+01  1.973062e+01  5.781534e+00   \n",
            "std    2.959734e+00         1.679374e+02  5.692963e+00  2.200617e+00   \n",
            "min    5.429000e+02         0.000000e+00 -2.500000e+00 -1.480000e+01   \n",
            "25%    1.017660e+03         0.000000e+00  1.370000e+01  4.500000e+00   \n",
            "50%    1.018750e+03         0.000000e+00  2.360000e+01  4.500000e+00   \n",
            "75%    1.018750e+03         0.000000e+00  2.360000e+01  8.000000e+00   \n",
            "max    1.033770e+03         1.407300e+03  3.830000e+01  1.820000e+01   \n",
            "\n",
            "             ddInst        ffInst  \n",
            "count  1.677450e+07  1.677450e+07  \n",
            "mean   1.975433e+02  4.053375e+00  \n",
            "std    5.633611e+01  1.529742e+00  \n",
            "min    0.000000e+00  0.000000e+00  \n",
            "25%    2.113000e+02  4.700000e+00  \n",
            "50%    2.113000e+02  4.700000e+00  \n",
            "75%    2.113000e+02  4.700000e+00  \n",
            "max    3.600000e+02  2.180000e+01   \n",
            "\n",
            "Temperatura: min=-2.5¬∞C | max=38.3¬∞C\n",
            "Humedad relativa: min=6.3% | max=100.0%\n",
            "\n",
            "======================\n",
            "FIN DE VERIFICACI√ìN DEL DATASET\n",
            "======================\n",
            "\n",
            "--- Remuestreando los datos a frecuencia Horaria (h) ---\n",
            "--- Extrayendo caracter√≠sticas temporales (Hora y D√≠a del A√±o) ---\n",
            "Modo: CON Lags (ts_lag_1h, ts_lag_2h, ts_lag_3h...)\n",
            "\n",
            "--- Caracter√≠sticas (X) a usar en el modelo ---\n",
            "['hr', 'p0', 'hour', 'dayofyear', 'tslag1h', 'tslag2h', 'tslag3h', 'hrlag1h', 'hrlag2h', 'hrlag3h', 'p0lag1h', 'p0lag2h', 'p0lag3h']\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Divisi√≥n de Datos: 60% Train | 20% Validation | 20% Test ---\n",
            "Train:       35899 filas (60.0%)\n",
            "Validation:  11966 filas (20.0%)\n",
            "Test:        11968 filas (20.0%)\n",
            "Total:       59833 filas\n",
            "\n",
            "--- Entrenando Random Forest Regressor ---\n",
            "Hiperpar√°metros: {'max_depth': 20, 'min_samples_leaf': 5, 'n_estimators': 100}\n",
            "\n",
            "============================================================\n",
            "M√âTRICAS - TRAIN SET\n",
            "============================================================\n",
            "MAE  (Mean Absolute Error):       0.0850 ¬∞C\n",
            "MSE  (Mean Squared Error):        0.0150 ¬∞C¬≤\n",
            "RMSE (Root Mean Squared Error):   0.1225 ¬∞C\n",
            "R¬≤   (Coeficiente Determinaci√≥n): 0.9936\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "M√âTRICAS - VALIDATION SET\n",
            "============================================================\n",
            "MAE  (Mean Absolute Error):       0.2467 ¬∞C\n",
            "MSE  (Mean Squared Error):        0.1275 ¬∞C¬≤\n",
            "RMSE (Root Mean Squared Error):   0.3571 ¬∞C\n",
            "R¬≤   (Coeficiente Determinaci√≥n): 0.9553\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "M√âTRICAS - TEST SET\n",
            "============================================================\n",
            "MAE  (Mean Absolute Error):       0.3408 ¬∞C\n",
            "MSE  (Mean Squared Error):        0.2119 ¬∞C¬≤\n",
            "RMSE (Root Mean Squared Error):   0.4604 ¬∞C\n",
            "R¬≤   (Coeficiente Determinaci√≥n): 0.9380\n",
            "============================================================\n",
            "\n",
            "--- üîç An√°lisis de Overfitting ---\n",
            "Diferencia R¬≤ (Train - Val):  0.0382\n",
            "Diferencia MAE (Val - Train): 0.1616 ¬∞C\n",
            "No se detecta overfitting significativo\n",
            "\n",
            "Modelo guardado como: random_forest_H1_hourly_full_features.joblib\n",
            "Lista de features guardada como: features_H1_hourly_full_features.joblib\n",
            "\n",
            "--- Importancia de las Caracter√≠sticas (Feature Importance) ---\n",
            "tslag1h      0.850195\n",
            "hour         0.097693\n",
            "tslag3h      0.026731\n",
            "hr           0.009526\n",
            "dayofyear    0.005142\n",
            "hrlag1h      0.002162\n",
            "hrlag3h      0.002090\n",
            "hrlag2h      0.002049\n",
            "tslag2h      0.001634\n",
            "p0           0.001029\n",
            "p0lag3h      0.000635\n",
            "p0lag1h      0.000585\n",
            "p0lag2h      0.000528\n",
            "dtype: float64\n",
            "\n",
            "--- Validaci√≥n Cruzada Temporal (Time Series CV sobre Train Set) ---\n",
            "Fold 1: MAE = 0.1933¬∞C | MSE = 0.0831 | R¬≤ = 0.9623\n",
            "Fold 2: MAE = 0.1564¬∞C | MSE = 0.0469 | R¬≤ = 0.9784\n",
            "Fold 3: MAE = 0.1555¬∞C | MSE = 0.0463 | R¬≤ = 0.9763\n",
            "Fold 4: MAE = 0.1288¬∞C | MSE = 0.0309 | R¬≤ = 0.9836\n",
            "Fold 5: MAE = 0.1374¬∞C | MSE = 0.0359 | R¬≤ = 0.9859\n",
            "\n",
            "Promedio CV: MAE = 0.1543¬∞C ¬± 0.0222\n",
            "Promedio CV: MSE = 0.0486 ¬± 0.0183\n",
            "Promedio CV: R¬≤  = 0.9773 ¬± 0.0082\n",
            "\n",
            "--- Validaci√≥n por Estaciones del A√±o (Test Set) ---\n",
            "Invierno    : MAE = 0.2977¬∞C | MSE = 0.1602 | R¬≤ = 0.9367 (2160 muestras)\n",
            "Primavera   : MAE = 0.3135¬∞C | MSE = 0.1732 | R¬≤ = 0.9280 (2208 muestras)\n",
            "Verano      : MAE = 0.4069¬∞C | MSE = 0.3047 | R¬≤ = 0.8154 (3952 muestras)\n",
            "Oto√±o       : MAE = 0.3113¬∞C | MSE = 0.1655 | R¬≤ = 0.9318 (3648 muestras)\n",
            "\n",
            "--- Detecci√≥n de Predicciones en Condiciones Extremas (Test Set) ---\n",
            "‚ùÑ  Fr√≠o extremo (<18.1¬∞C): MAE = 0.3550¬∞C | MSE = 0.2487 | R¬≤ = 0.5098 (4712 casos)\n",
            "Calor extremo (>22.3¬∞C): MAE = 0.1616¬∞C | MSE = 0.0432 | R¬≤ = 0.7201 (653 casos)\n",
            "\n",
            "--- An√°lisis de Sesgo Temporal en Errores (Test Set) ---\n",
            "\n",
            "Error promedio por hora del d√≠a:\n",
            "        mean     std\n",
            "hour                \n",
            "0    -0.2827  0.2536\n",
            "1    -0.2072  0.2274\n",
            "2    -0.1897  0.2276\n",
            "3    -0.1868  0.2330\n",
            "4    -0.1654  0.2456\n",
            "5    -0.1449  0.2571\n",
            "6    -0.1475  0.2656\n",
            "7    -0.1300  0.2695\n",
            "8    -0.1203  0.2828\n",
            "9    -0.0244  0.3317\n",
            "10    0.1438  0.6049\n",
            "11    0.1261  0.7221\n",
            "12    0.1621  0.6968\n",
            "13    0.2574  0.5475\n",
            "14    0.2258  0.4200\n",
            "15    0.1423  0.3584\n",
            "16    0.0618  0.3367\n",
            "17    0.0068  0.3276\n",
            "18   -0.0580  0.3021\n",
            "19   -0.1414  0.2937\n",
            "20   -0.2884  0.3518\n",
            "21   -0.4642  0.4282\n",
            "22   -0.5418  0.3866\n",
            "23   -0.5221  0.4359\n",
            "\n",
            "Horas con sesgo >0.5¬∞C: [22, 23]\n",
            "\n",
            "--- Generando Gr√°ficos ---\n",
            "Gr√°ficos guardados exitosamente\n",
            "\n",
            "Archivos generados:\n",
            "  - random_forest_H1_hourly_full_features.joblib\n",
            "  - features_H1_hourly_full_features.joblib\n",
            "  - prediccion_test_H1.png\n",
            "  - feature_importance_H1.png\n",
            "  - scatter_test_H1.png\n",
            "  - metricas_comparacion_H1.png\n"
          ]
        }
      ]
    }
  ]
}